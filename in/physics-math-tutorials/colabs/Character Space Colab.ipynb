{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Character Space Colab","provenance":[{"file_id":"1ZyYcaSTw0zCxPMBkxLkpPBzf29oVNDm_","timestamp":1636425983925},{"file_id":"1-p1uxsGX5NYyvEXDXJ4XMZv9yLR271qI","timestamp":1615393780332}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1yJ-de_VaSum"},"source":["Copyright 2021 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","    https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"_Qs1SELUrds-"},"source":["# Character Space - The `ascii_letters` vector space\n","\n","Inspired by the char2vec colab."]},{"cell_type":"markdown","metadata":{"id":"dwsIY80YrqhA"},"source":["## The `character` vector space\n","\n","We begin by defining the `character` vector space which has the `ascii_letters` as its `basis`.  Why ascii? It's arbitrary and having only __ dimesions keeps things simple. We could have taken any of the unicode variations, but depending on the size of the character space we may want to pursue some optional optimizations which are also discussed below.\n","\n","There are 2 functions which compute: <br>\n","1) the `vector` representation of the word, i.e. the counts of the characters in the word <br>\n","2) the `support`: the set of unique characters"]},{"cell_type":"code","metadata":{"id":"ULr9_uRXqhq_"},"source":["from collections import Counter\n","import math\n","\n","def char2vec(word):\n","  # Counts each of the the characters in word.\n","  # We use a dictionary instead of a sparse matrix to describe the characters,\n","  # however the concept is identical.\n","  return Counter(word)\n","\n","def support(v):\n","  # The support of a vector over a basis is the subset of basis elements with\n","  # non-zero components.\n","  return set(v)\n","\n","# Note: We could have written this simpler: char2vec = Counter; support = set;"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9NR0cM7rbFm"},"source":["# Check Point\n","1. What is the dimension of the ascii character vector space?\n","1. What is the support of 'pizza'?"]},{"cell_type":"code","metadata":{"id":"Yyw_-Fxvrf89","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615490814886,"user_tz":360,"elapsed":580,"user":{"displayName":"Gabe Gaster","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioy7CZ5VKbOFBRdDiAwPuqjurDum3Fg0CLFi7t=s64","userId":"01134821983866106427"}},"outputId":"e5478eb7-2634-47f1-aa38-a7b34b60e5be"},"source":["import string\n","\n","vector_space = string.ascii_letters\n","\n","print(f\"1. The dimension of vector_space is {len(vector_space)}, since there is one \\n   independent vector per character and that's all of them.\\n\")\n","print(f'2. The support of pizza is {support(char2vec(\"pizza\"))}.')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1. The dimension of vector_space is 52, since there is one \n","   independent vector per character and that's all of them.\n","\n","2. The support of pizza is {'a', 'z', 'p', 'i'}.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U-StydByPbM6"},"source":["# The `dot` product and the `norm`\n","\n","Convince yourself that this dot product here corresponds to dotting vectors.\n","\n","With `numpy` arrays, that is `numpy.dot`."]},{"cell_type":"code","metadata":{"id":"Df_nFOA6PGDr"},"source":["import string\n","\n","def dot(v, w):\n","  domain = string.ascii_letters\n","  # Note that this computation is equivalent to each of the following\n","  # optimizations. Exercise: Why?\n","  #   domain = support(v).union(w)\n","  #   domain = support(v).intersection(w)\n","  #\n","  # This domain here bears a passing resemblance to integration doesn't it.\n","  return sum(v[ch] * w[ch] for ch in domain) \n","\n","def norm(v):\n","  return math.sqrt(dot(v, v))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Gyd25mNPhud"},"source":["# An Example"]},{"cell_type":"code","metadata":{"id":"cJnTdgqBETlS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615490815267,"user_tz":360,"elapsed":940,"user":{"displayName":"Gabe Gaster","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioy7CZ5VKbOFBRdDiAwPuqjurDum3Fg0CLFi7t=s64","userId":"01134821983866106427"}},"outputId":"4ccd23f0-217a-442f-e9a2-a3b1267fe534"},"source":["# Our sample \"words\"\n","wordlist = [\n","  'TheQuickBrownFoxJumpsOverTheLazyDog',\n","  'TheQuickWhiteFoxJumpsOverTheLazyDog',\n","  'SupermanJumpsOverTheTallBuilding',\n","]\n","\n","# For each of our words, print its vector and some information about it.\n","for word in wordlist:\n","  print(word)\n","  print(\"  vector: \", char2vec(word))\n","  print(\"  support: \", support(char2vec(word)))\n","  print(\"  norm:\", norm(char2vec(word)), end=\"\\n\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TheQuickBrownFoxJumpsOverTheLazyDog\n","  vector:  Counter({'e': 3, 'o': 3, 'T': 2, 'h': 2, 'u': 2, 'r': 2, 'Q': 1, 'i': 1, 'c': 1, 'k': 1, 'B': 1, 'w': 1, 'n': 1, 'F': 1, 'x': 1, 'J': 1, 'm': 1, 'p': 1, 's': 1, 'O': 1, 'v': 1, 'L': 1, 'a': 1, 'z': 1, 'y': 1, 'D': 1, 'g': 1})\n","  support:  {'D', 'a', 'v', 'g', 'r', 'J', 'h', 's', 'p', 'B', 'k', 'w', 'i', 'm', 'x', 'z', 'Q', 'F', 'y', 'o', 'n', 'c', 'O', 'L', 'u', 'T', 'e'}\n","  norm: 7.416198487095663\n","\n","TheQuickWhiteFoxJumpsOverTheLazyDog\n","  vector:  Counter({'e': 4, 'h': 3, 'T': 2, 'u': 2, 'i': 2, 'o': 2, 'Q': 1, 'c': 1, 'k': 1, 'W': 1, 't': 1, 'F': 1, 'x': 1, 'J': 1, 'm': 1, 'p': 1, 's': 1, 'O': 1, 'v': 1, 'r': 1, 'L': 1, 'a': 1, 'z': 1, 'y': 1, 'D': 1, 'g': 1})\n","  support:  {'D', 'a', 'v', 'g', 'r', 'J', 'h', 's', 'p', 't', 'k', 'i', 'm', 'x', 'z', 'Q', 'W', 'F', 'y', 'o', 'c', 'O', 'L', 'u', 'T', 'e'}\n","  norm: 7.810249675906654\n","\n","SupermanJumpsOverTheTallBuilding\n","  vector:  Counter({'u': 3, 'e': 3, 'l': 3, 'p': 2, 'r': 2, 'm': 2, 'a': 2, 'n': 2, 'T': 2, 'i': 2, 'S': 1, 'J': 1, 's': 1, 'O': 1, 'v': 1, 'h': 1, 'B': 1, 'd': 1, 'g': 1})\n","  support:  {'d', 'a', 'v', 'g', 'r', 'J', 'l', 'h', 's', 'p', 'B', 'i', 'S', 'm', 'n', 'O', 'u', 'T', 'e'}\n","  norm: 8.0\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q31yqmk2tbvo"},"source":["## The angles between vectors\n","This function finds the cosine_similarity in character-space between two char2vec vectors."]},{"cell_type":"code","metadata":{"id":"OsMX6TWftatM"},"source":["def cosine_similarity(v, w):\n","  return dot(v, w) / norm(v) / norm(w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_damA22tmlg"},"source":["## How far are our examples from one another?"]},{"cell_type":"code","metadata":{"id":"e79Ilvb-_3WX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615490815286,"user_tz":360,"elapsed":934,"user":{"displayName":"Gabe Gaster","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioy7CZ5VKbOFBRdDiAwPuqjurDum3Fg0CLFi7t=s64","userId":"01134821983866106427"}},"outputId":"50a6ceaf-cc80-4c81-f458-1572b67c02c2"},"source":["from itertools import combinations\n","\n","# Find the cosine similarity between each of the 3 vectors created above\n","# Similar senteces will have higher scores, ranging from 0-1.\n","for x, y in combinations(wordlist, 2):\n","  print (\"Cosine Similarity between\", x, \"and\", y,\"=\",\n","         cosine_similarity(char2vec(x),char2vec(y)), end=\"\\n\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cosine Similarity between TheQuickBrownFoxJumpsOverTheLazyDog and TheQuickWhiteFoxJumpsOverTheLazyDog = 0.9150179365143998\n","\n","Cosine Similarity between TheQuickBrownFoxJumpsOverTheLazyDog and SupermanJumpsOverTheTallBuilding = 0.6910548590248231\n","\n","Cosine Similarity between TheQuickWhiteFoxJumpsOverTheLazyDog and SupermanJumpsOverTheTallBuilding = 0.6721936196477039\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pn9H8uyrzFsh"},"source":["---\n","\n","# Totally Optional Exercises\n","\n","1. If the cosine_similarity=0, what do you know about the words?\n","1. If the cosine_similarity=1, are the words the same?\n","1. Can the cosine_similarity be negative?\n","1. Is the cosine_similarity symmetric?\n","1. In the definition of `dot`, above, why are these three definitions of the variable `domain` equivalent?\n","  - `domain = string.ascii_letters`\n","  - `domain = support(v).union(w)`\n","  - `domain = support(v).intersection(w)`\n","1. Define a `distance` by taking the inverse cosine of `cosine_similarity`.\n","  - Show this now actually computes the angle.\n","  - Given 3 words, does this notion of `distance` satisfy the triangle inequality? Modify the code above to show these three words do.\n","\n","##### **Advanced**\n","1. Find a sequence of words w_1, w_2, ..., for which the sequence norm(w_1), norm(w_2), ... is [unbounded](https://en.wikipedia.org/wiki/Bounded_function).\n","1. Find two sequences of words whose pair-wise cosine_similarity is arbitrarily close to 1. i.e. Find word sequences a=a_1,a_2,... and b=b_1,b_2,... so that a_n and b_n make arbitrarily tiny angles.\n"," - This means that our vector space (with this distance) is not [topologically discrete](https://en.wikipedia.org/wiki/Discrete_space).\n"," - State a reasonable condition so that the vector space is discrete."]},{"cell_type":"code","metadata":{"id":"PNlBMjWbB0Jn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615490815287,"user_tz":360,"elapsed":916,"user":{"displayName":"Gabe Gaster","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioy7CZ5VKbOFBRdDiAwPuqjurDum3Fg0CLFi7t=s64","userId":"01134821983866106427"}},"outputId":"1e741833-2823-4421-8210-86547127ec61"},"source":["def similarity(x,y):\n","  return cosine_similarity(char2vec(x), char2vec(y))\n","\n","print(\"similarity of\", \"able\", \"elba\", \": \", similarity(\"able\", \"elba\"))\n","print(\"similarity of\", \"gabe\", \"juno\", \": \", similarity(\"gabe\", \"juno\"))\n","print(\"similarity of\", \"piz...za\", \"piz...zza\", \": \", similarity(\"pi\" + \"z\"*10**3 + \"a\", \"pi\" + \"z\"*(10**3 +1) + \"a\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["similarity of able elba :  1.0\n","similarity of gabe juno :  0.0\n","similarity of piz...za piz...zza :  0.999999999998503\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OdnYbzg6Qc7a"},"source":["---\n","\n","# Forget about sparsity! (a **dense** implementation)\n","\n","The above example used a Python Counter, which is a `defaultdict(int)` to implement sparse vectors. That is critically important if your domain is e.g. the English Language. However in our case here our domain is just characters, so the sparseity is unnecessary. Here is a non-sparse, i.e. dense, implementation."]},{"cell_type":"code","metadata":{"id":"OGqqjXs6E8ti"},"source":["import string\n","import numpy as np\n","\n","vector_space = string.ascii_letters\n","\n","def char2index(ch):\n","  return vector_space.index(ch)\n","def index2char(index):\n","  return vector_space[index]\n","\n","def char2vec_dense(word):\n","  out = np.zeros(len(vector_space))\n","  for ch in word:\n","    out[char2index(ch)] += 1\n","  return out\n","\n","def support_dense(v):\n","  return \"\".join(index2char(i) for i in v.nonzero()[0])\n","\n","# Now the dot, really is np.dot\n","def dot_dense(v, w):\n","  return v.dot(w)\n","\n","def norm_dense(v):\n","  return dot_dense(v)**.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2LEx-EZLl6g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615490815292,"user_tz":360,"elapsed":788,"user":{"displayName":"Gabe Gaster","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioy7CZ5VKbOFBRdDiAwPuqjurDum3Fg0CLFi7t=s64","userId":"01134821983866106427"}},"outputId":"af3a4e16-1ce8-4e96-d5b6-942cb5b1b5ed"},"source":["# For each of our words, print its vector and some information about it.\n","for word in wordlist:\n","  print(word)\n","  print(\"  vector: \", char2vec_dense(word))\n","  print(\"  support: \", support_dense(char2vec_dense(word)))\n","  print(\"  norm:\", norm_dense(char2vec_dense(word)), end=\"\\n\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TheQuickBrownFoxJumpsOverTheLazyDog\n","  vector:  [1. 0. 1. 0. 3. 0. 1. 2. 1. 0. 1. 0. 1. 1. 3. 1. 0. 2. 1. 0. 2. 1. 1. 1.\n"," 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 2. 0. 0.\n"," 0. 0. 0. 0.]\n","  support:  aceghikmnoprsuvwxyzBDFJLOQT\n","  norm: 7.416198487095663\n","\n","TheQuickWhiteFoxJumpsOverTheLazyDog\n","  vector:  [1. 0. 1. 0. 4. 0. 1. 3. 2. 0. 1. 0. 1. 0. 2. 1. 0. 1. 1. 1. 2. 1. 0. 1.\n"," 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 2. 0. 0.\n"," 1. 0. 0. 0.]\n","  support:  aceghikmoprstuvxyzDFJLOQTW\n","  norm: 7.810249675906654\n","\n","SupermanJumpsOverTheTallBuilding\n","  vector:  [2. 0. 0. 1. 3. 0. 1. 1. 2. 0. 0. 3. 2. 2. 0. 2. 0. 2. 1. 0. 3. 1. 0. 0.\n"," 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 2. 0. 0.\n"," 0. 0. 0. 0.]\n","  support:  adeghilmnprsuvBJOST\n","  norm: 8.0\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JQo4p8PDp8Ci"},"source":["# More exercises\n","1. Hey, vector-spaces are over a field. What is the field in which this vector-space is over?\n","1. Pick a unicode encoding. What is its dimension?\n","1. Find an alternative basis for ascii character space. Can you think of a situation where it might be more useful than `string.ascii_letters`? (*)\n","1. Implement cosine_similarity_dense. It looks an awful lot like cosine_similarity, doesn't it?\n","1. Re-implement `cosine_similarity` using `sklearn.CountVectorizer`.\n","1. What is an advantage of the sparse implementation?\n","1. What is an advantage of the dense implementation?\n"]},{"cell_type":"code","metadata":{"id":"JQatRI57xJhR"},"source":["# This function looks an awful lot like cosine_distance, doesn't it.\n","def cosine_similarity_dense(v, w):\n","  return dot_dense(v, w) / norm_dense(v) / norm_dense(w)"],"execution_count":null,"outputs":[]}]}