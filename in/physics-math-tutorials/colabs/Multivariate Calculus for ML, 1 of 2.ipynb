{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multivariate Calculus for ML, 1 of 2.ipynb","provenance":[{"file_id":"1qwRu90UyHfwZwDecvEYXkDs6bCiq_Aw_","timestamp":1636426151199},{"file_id":"/piper/depot/google3/identity/metrics/analysis/tools/colab/idam_snippets.ipynb","timestamp":1619126330459},{"file_id":"/piper/depot/google3/identity/metrics/analysis/tools/colab/idam_snippets.ipynb","timestamp":1595525197267},{"file_id":"/piper/depot/google3/identity/metrics/analysis/tools/colab/idam_snippets.ipynb","timestamp":1586906003153},{"file_id":"/piper/depot/google3/identity/metrics/analysis/tools/colab/idam_snippets.ipynb","timestamp":1585782327201},{"file_id":"/piper/depot/google3/identity/metrics/analysis/tools/colab/idam_snippets.ipynb","timestamp":1584576334768},{"file_id":"/piper/depot/google3/identity/metrics/analysis/tools/colab/idam_snippets.ipynb","timestamp":1582846550631},{"file_id":"1WLzymB-YoYxRAGuO2M1mJKTkT4bZb9j3","timestamp":1582311562092},{"file_id":"1PkdYII-rPXNHA0ZLUI58_txrpmAWcHG-","timestamp":1582259355186},{"file_id":"1HKgXVZcMoxxILmf6ICBFMi21WO_L1fos","timestamp":1562012436876},{"file_id":"1BapNzjW6vy7-v-h7dyz9pUsXIKVY08Ud","timestamp":1533764843423},{"file_id":"1qEIK-Nudt9im4RrkiQZKVrb62w2uG-sH","timestamp":1533764375026},{"file_id":"1B-yURMDT8SfRAqjpagSrqwX3sPMg3mkQ","timestamp":1529603510318},{"file_id":"1LrYiLqa-pEHZd1ejukkFm5XOk-iSAO2T","timestamp":1522709615737},{"file_id":"1yDXiGoXTzTpnAVJVFUpEI1PDkV4W96al","timestamp":1519152157646},{"file_id":"1YBVN2ELdpPx4DdoB270nW_vzf5drrHYe","timestamp":1518026490591},{"file_id":"1Ob1sjNFQrtyfWYP4vKQDFrdO6UtgRB1D","timestamp":1517436304391},{"file_id":"1x4cYe6NkEM22G0kyif9usftKxgLdyFtb","timestamp":1516736325070},{"file_id":"1LulBX9RKQyen2oTX6SbxcczVrcTGs09B","timestamp":1515781182986},{"file_id":"0B-S1fcbN9Vu1ak9MSEl2NENBdUk","timestamp":1508776854844},{"file_id":"0B-S1fcbN9Vu1dFdwSE15WkVIb1U","timestamp":1506709905334},{"file_id":"0B_38idL6wbpJUUdVVmstQnQ2aUE","timestamp":1506196249428}],"collapsed_sections":[],"last_runtime":{"build_target":"//learning/deepmind/dm_python:dm_notebook3","kind":"private"},"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"a7bGBVfKZIit"},"source":["Copyright 2021 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","    https://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"QMTMq4J7i7Ac"},"source":["**Make a copy of this notebook!**"]},{"cell_type":"code","metadata":{"id":"bowOMK0cUlOz"},"source":["#@title Python imports\n","import collections\n","import datetime\n","from functools import partial\n","import math\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from scipy import stats\n","import seaborn as sns\n","\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"13bweqwnnXGi"},"source":["# Make colab plots larger\n","plt.rcParams['figure.figsize'] = [8, 6]\n","plt.rcParams['figure.dpi'] = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDdFdzBRC4EV"},"source":["# Section 1\n","\n","In this section we'll cover:\n","\n","* Using Jax\n","* Computing derivatives with Jax\n","* Gradient Descent (single variable)\n","* Newton's Method (single variable)\n"]},{"cell_type":"markdown","metadata":{"id":"utdsyxxHt1gQ"},"source":["## Jax\n","\n","[Autodiff cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\n","\n","Jax is a python library that provides the ability to differentiate many python\n","functions.\n"]},{"cell_type":"code","metadata":{"id":"6njnziJwuaxM"},"source":["import jax\n","import jax.numpy as jnp\n","from jax import jit, grad, vmap, api, random, jacfwd, jacrev\n","from jax.experimental import optimizers, stax"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSA_bOyV5FoI"},"source":["## Basic autodiff example\n","\n","Let's take the derivative of $y = f(x) = x^2$, which we know to be $f'(x) = 2x$.\n","\n"]},{"cell_type":"code","metadata":{"id":"fPJuU93sc1G2"},"source":["def square(x):\n","  return x * x\n","\n","# Compute the derivative with grad from Jax\n","dsquare = grad(square)\n","\n","# Plot the function and the derivative\n","domain = np.arange(0, 2.5, 0.1)\n","plt.plot(domain, square(domain), label=\"$y=x^2$\")\n","plt.plot(domain, list(map(dsquare, domain)), label=\"$y'=\\\\frac{dy}{dx} = 2x$\")\n","# Note can also use JAX's vmap: vmap(dsquare)(domain) instead of list(map(dsquare, domain))\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2etbsAv7vOjj"},"source":["### Exercise\n","\n","Write a function to compute the sigmoid\n","\n","$$f(x) = \\frac{1}{1 + e^{-x}}$$\n","\n","using `jnp.exp` to compute $e^x$ -- need to use the Jax version of\n","numpy for autodiff to work.\n","\n","Then verify that\n","\n","$$ f'(x) = f(x) (1 - f(x))$$\n","\n","For example, compare some explicit values and plot the differences between the derivative and $f(x) (1 - f(x))$."]},{"cell_type":"code","metadata":{"id":"JY9kppZbymC5"},"source":["## Your code here\n","\n","# Compute the sigmoid\n","def sigmoid(x):\n","  pass\n","\n","# Compute the derivative (using grad)\n","\n","# Compare derivative values to f(x) * (1 - f(x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWl097F62z1M"},"source":["# @title Solution (double-click to show)\n","\n","# Compute the sigmoid\n","def sigmoid(x):\n","  return 1. / (1. + jnp.exp(-x))\n","\n","# Compute the derivative (using grad)\n","deriv = grad(sigmoid)\n","\n","# Compare derivative values to f(x) * (1 - f(x))\n","xs = np.arange(-3, 3.1, 0.1)\n","ys = []\n","for x in xs:\n","  ys.append(deriv(x) - sigmoid(x) * (1. - sigmoid(x)))\n","plt.scatter(xs, ys)\n","plt.title(\"Differences between Jax derivative and $f(x)(1-f(x))$\\nNote the scale modifier in the upper left\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YaZLtaFLwD2D"},"source":["# Single variable gradient descent examples\n","\n","In this example we solve $x^2=x$, which we know has two solutions. Different\n","initial starting points yield convergence to different solutions, or\n","non-convergence to either solution at $x_0 = 1/2$.\n","\n","We need to turn the problem into one of finding local extrema. So we consider\n","the function $f(x) = \\left( x^2 - x\\right)^2$, which is differentiable and\n","has local minimum at $x=0$ and $x=1$. We square so that the points where\n","$x^2=x$ are minima instead of zeros of a function like $g(x) = x^2 - x$.\n","\n","Notice in the plot below that the function also has a local maximum at $x=1/2$,\n","which is centered between the two solutions. Intuitively, gradient descent\n","starting at $x_0=1/2$ will not move because there's no reason to favor either\n","local minumum.\n","\n","Let's plot the function first:"]},{"cell_type":"code","metadata":{"id":"_eK5vxzoBeZl"},"source":["def f(x):\n","  return (x**2 - x)**2\n","\n","xs = np.arange(-1, 2.05, 0.05)\n","ys = f(xs)\n","plt.plot(xs, ys)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvbDrC5m0qtK"},"source":["Let's define a function to compute iterations of gradient descent.\n","\n","$$\\begin{eqnarray}\n","x_{n+1} &=& x_n - \\alpha f'(x_n) \\\\\n","\\end{eqnarray}$$\n"]},{"cell_type":"code","metadata":{"id":"AHwj8Jj-eFf3"},"source":["def gradient_descent(dfunc, x0, iterations=100, alpha=0.1):\n","  \"\"\"dfunc is the derivative of the function on which we\n","  perform descent.\"\"\"\n","  xs = [x0]\n","  for i in range(iterations):\n","    x = xs[-1]\n","    x = x - alpha * dfunc(float(x))\n","    xs.append(x)\n","  return xs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0I-wITXo7Us"},"source":["# Let's try it on our function f now.\n","\n","# Compute the derivative\n","df = grad(f)\n","\n","# Try some different starting points.\n","for x0 in [0.25, 0.5, 0.50001, 0.85]:\n","  xs = gradient_descent(df, x0, iterations=30, alpha=1.)\n","  plt.plot(range(len(xs)), xs, label=x0)\n","plt.xlabel(\"iterations\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CKP5Mr21jclP"},"source":["### Check your understanding: Explain what happened with each of the four curves in the plot."]},{"cell_type":"markdown","metadata":{"id":"afrXBh6H1elL"},"source":["### Exercise: What happens if we decrease the learning rate $\\alpha$?\n","Recreate the plot above using $\\alpha=0.1$ instead."]},{"cell_type":"code","metadata":{"id":"X0G3IzHg1VjU"},"source":["## Your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0lC6Pa1BIjb"},"source":["## Example 2\n","\n","In this example we use gradient descent to approximate $\\sqrt{3}$. We use\n","the function $f(x) = \\left(x^2 - 3\\right)^2$ and construct a sequence converging\n","to the positive solution. In this case notice the impact of the learning rate\n","both on the time to convergence and whether the convergence is monotonic or\n","oscillitory. Larger learning rates such as $\\alpha=1$ can cause divergence."]},{"cell_type":"code","metadata":{"id":"IP6nhyqCp0se"},"source":["def f2(x):\n","  return (x*x - 3)**2\n","\n","df = grad(f2)\n","\n","x0 = 2\n","for alpha in [0.08, 0.01]:\n","  xs = gradient_descent(df, x0, iterations=40, alpha=alpha)\n","  plt.plot(range(len(xs)), xs, label=\"$\\\\alpha = {}$\".format(alpha))\n","plt.xlabel(\"iterations\")\n","\n","# Plot the correct value\n","sqrt3 = math.pow(3, 0.5)\n","n = len(xs)\n","plt.plot(range(n), [sqrt3]*n, label=\"$\\\\sqrt{3}$\", linestyle=\"--\")\n","\n","plt.legend()\n","plt.show()\n","print(\"Sqrt(3) =\", sqrt3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XD4-ikrTC2os"},"source":["## Exercise\n","\n","Solve the equation $x = e^{-x}$, which does not have an easily obtainable solution algebraically.\n","\n","The solution is approximately $x = 0.567$. Again note the impact of\n","the learning rate.\n","\n","Use `jnp.exp` for the exponential function.\n"]},{"cell_type":"code","metadata":{"id":"FUuV0TI-q4wT"},"source":["def f3(x):\n","  \"\"\"Define the function f(x) = (x - e^(-x))^2.\"\"\"\n","  ## Your code here\n","  pass\n","\n","# Compute the gradient\n","\n","# Initial guess\n","x0 = 0.4\n","\n","## Add code here for gradient descent using the functions above\n","\n","## Plot the gradient descent values\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"VN6hgAKZ5P2E"},"source":["#@title Solution (double click to expand)\n","\n","def f3(x):\n","  \"\"\"Define the function f(x) = (x - e^(-x))^2.\"\"\"\n","  return (x - jnp.exp(-x))**2\n","\n","# Compute the gradient\n","df = grad(f3)\n","\n","# Initial guess\n","x0 = 0.4\n","for alpha in [0.01, 0.1]:\n","  xs = gradient_descent(df, x0, iterations=50, alpha=alpha)\n","  plt.plot(range(len(xs)), xs, label=\"$\\\\alpha = {}$\".format(alpha))\n","plt.xlabel(\"iterations\")\n","\n","plt.legend()\n","plt.show()\n","\n","print(\"Final iteration:\", xs[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"65z4WrwIEWdj"},"source":["# Newton's method, single variable\n","\n","We can use Newton's\n","method to find zeros of functions. Since local extrema occur at zeros of\n","the derivative, we can apply Newton's method to the first derivative to obtain\n","a second-order alternative to gradient descent.\n","\n","$$\\begin{eqnarray}\n","x_{n+1} &=& x_n - \\alpha \\frac{f'(x_n)}{f''(x_n)} \\\\\n","\\end{eqnarray}$$ \n"]},{"cell_type":"code","metadata":{"id":"TTVT0U4xsJTf"},"source":["def newtons_method(func, x0, iterations=100, alpha=1.):\n","  dfunc = grad(func)\n","  xs = [x0]\n","  for i in range(iterations):\n","    x = xs[-1]\n","    x = x - alpha * func(x) / dfunc(float(x))\n","    xs.append(x)\n","  return xs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_W5aEta33xEL"},"source":["Let's repeat the example of finding the value of $\\sqrt{3}$ with Newton's method and compare to gradient descent.\n","\n","For small $\\alpha$, gradient descent seems to perform better:"]},{"cell_type":"code","metadata":{"id":"D_KuyPxwFOom"},"source":["def f(x):\n","  return (x**2 - 3)**2\n","\n","# Let's make a function we can reuse\n","def compare_gradient_newton(func, x0, alpha=0.01, iterations=50):\n","  # Compute the first and second derivatives\n","  df = grad(func)\n","\n","  # Compute Newton's method iterations\n","  xs = newtons_method(df, x0, alpha=alpha, iterations=iterations)\n","\n","  # Compute gradient descent with same alpha\n","  xs2 = gradient_descent(df, x0, alpha=alpha, iterations=iterations)\n","\n","  # Plot it all\n","  plt.plot(range(len(xs2)), xs2, label=\"Gradient Descent\")\n","  plt.plot(range(len(xs)), xs, label=\"Newton's method\")\n","  plt.xlabel(\"iterations\")\n","  plt.legend()\n","  plt.title(\"$\\\\alpha = {}$\".format(alpha))\n","\n","  # Plot the solution\n","  sqrt3 = math.pow(3, 0.5)\n","  n = len(xs)\n","  plt.plot(range(n), [sqrt3]*n, label=\"$\\\\sqrt{3}$\", linestyle=\"--\")\n","\n","  plt.show()\n","\n","compare_gradient_newton(f, 2., alpha=0.01, iterations=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0ZnKEwx5H-O"},"source":["But for larger $\\alpha$, Newton's method is better behaved and gradient descent fails to converge."]},{"cell_type":"code","metadata":{"id":"PEOO4RXG46kx"},"source":["compare_gradient_newton(f, 2., alpha=0.1, iterations=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UNtCSLhA6TOQ"},"source":["In this case we can also apply Newton's method with just the first derivative to find a zero of $x^2 - 3$, i.e. we don't have to look for a minimum of \n","$(x^2 - 3)^2$ since Newton's method can also find zeros of functions."]},{"cell_type":"code","metadata":{"id":"Sj6xNZ91Kmp_"},"source":["def f(x):\n","  return x**2 - 3\n","\n","xs = newtons_method(f, 2., alpha=0.5, iterations=10)\n","plt.plot(range(len(xs)), xs, label=\"$\\\\alpha = {}$\".format(alpha))\n","plt.xlabel(\"iterations\")\n","\n","# Plot the solution\n","sqrt3 = math.pow(3, 0.5)\n","n = len(xs)\n","plt.plot(range(n), [sqrt3]*n, label=\"$\\\\sqrt{3}$\", linestyle=\"--\")\n","\n","print(xs[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LsPn-_O6CtQm"},"source":["# Section 2\n","\n","Now we'll look at multivariate derivatives and gradient descent,\n","again using Jax."]},{"cell_type":"markdown","metadata":{"id":"zEcdKQkBvXYR"},"source":["## Multivariate derivatives\n","\n","$$f(x, y) = x y^2$$\n","\n","$$ \\nabla f = [y^2, 2 x y]$$"]},{"cell_type":"code","metadata":{"id":"uUr_8594vSMv"},"source":["def f(x, y):\n","  return x * y * y\n","\n","# Compute the partial derivatives with grad from Jax\n","# Use float as arguments, else Jax will complain\n","print(\"f(3, 1)=\", f(3., 1.))\n","\n","# argnums allows us to specify which variable to take the derivative of, positionally\n","print(\"Partial x derivative at (3, 1):\", grad(f, argnums=0)(3., 1.))\n","print(\"Partial y derivative at (3, 1):\", grad(f, argnums=1)(3., 1.))\n","\n","# We can get both partials at the same time\n","print(\"Gradient vector at (3, 1):\", grad(f, (0, 1))(3., 1.))\n","g = [float(z) for z in grad(f, (0, 1))(3., 1.)]\n","print(\"Gradient vector at (3, 1):\", g)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EYhFa_S0zq85"},"source":["We can plot some of the vectors of a gradient. Let's consider\n","$$f(x, y) = x^2 + y^2$$\n","The partial derivatives are\n","$$\\frac{\\partial f}{\\partial x} = 2x$$\n","\n","$$\\frac{\\partial f}{\\partial y} = 2y$$\n","\n","So the gradient is $$\\nabla f = [2x, 2y]^T$$"]},{"cell_type":"code","metadata":{"id":"xjtRd4etNAFv"},"source":["def f(x, y):\n","  return x * x + y * y\n","\n","partial_x = grad(f, argnums=0)\n","partial_y = grad(f, argnums=1)\n","\n","xs = np.arange(-1, 1.25, 0.25)\n","ys = np.arange(-1, 1.25, 0.25)\n","\n","plt.clf()\n","\n","# Compute and plot the gradient vectors\n","for x in xs:\n","  for y in ys:\n","    u = partial_x(x, y)\n","    v = partial_y(x, y)\n","    plt.arrow(x, y, u, v, length_includes_head=True,\n","              head_width=0.1)\n","plt.xlim(-3, 3)\n","plt.ylim(-3, 3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qh458PfCNAau"},"source":["## Jacobians and Hessians with Jax\n","\n","Let's verify some of the example from the slides.\n","\n","Let $f(x) = \\sum_i{x_i} = x_1 + \\cdots + x_n$. Then the gradient is $\\nabla f (x) = [1, \\ldots, 1]^T.$"]},{"cell_type":"code","metadata":{"id":"XgkrYZk5NPzy"},"source":["n = 4\n","def f(x):\n","  return sum(x)\n","\n","test_point = [1., 2., 3., 4.]\n","\n","print(\"x = \", test_point)\n","print(\"Gradient(x):\", [float(x) for x in grad(f)(test_point)])\n","\n","## Try other test points, even random ones:\n","test_point = np.random.rand(4)\n","print()\n","print(\"x = \", test_point)\n","print(\"Gradient(x):\", [float(x) for x in grad(f)(test_point)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PojH0N34Oz_u"},"source":["### Exercise\n","\n","Compute the gradient of the function that sums the squares of the elements of a vector."]},{"cell_type":"code","metadata":{"cellView":"form","id":"3m2DN_pyO6w6"},"source":["# @title Solution (double click to show)\n","def sum_squares(x):\n","  return jnp.dot(x, x)\n","\n","test_point = np.array([1., 2., 3., 4.])\n","\n","print(\"x = \", test_point)\n","print(\"Gradient(x):\", [float(x) for x in grad(sum_squares)(test_point)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwYVrJHWPbeg"},"source":["Let's try a Jacobian now. In the slides we saw that Jacobian of $f(\\mathbf{x}) = Ax$ is $A$. Let's verify with Jax.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Z8Q4d1bTPsEO"},"source":["A = np.array([[1., 2.], [3., 4.]])\n","\n","def f(x):\n","  return jnp.dot(A, x)\n","\n","x = np.array([1., 1.])\n","jacfwd(f)(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mq6fKrA3RDFr"},"source":["# Try some other matrices A and a 3x3 matrix\n","# Note that Jax handles larger matrices with the same code\n","# But you'll need a length 3 vector for x\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYtz5rRFQtKE"},"source":["We can compute the Hessian by taking the Jacobian twice. In this case, $f(\\mathbf{x}) = Ax$ is a linear function, so the second derivatives should all be zero."]},{"cell_type":"code","metadata":{"id":"cJOo0cYFRApz"},"source":["A = np.array([[1., 2.], [3., 4.]])\n","\n","def f(x):\n","  return jnp.dot(A, x)\n","\n","def hessian(f):\n","  return jacfwd(jacrev(f))\n","\n","x = np.array([1., 1.])\n","hessian(f)(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8HMRGbKQRI5E"},"source":["# Try some other matrices A and a 3x3 matrix\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5bo54sfXRJne"},"source":["### Exercise\n","\n","Now try to take derivatives of the function $f(\\mathbf{x}) = x \\cdot A x$. Hints:\n","* Is it scalar or vector-valued?\n","* Does it matter if $A$ is symmetric $(A = A^T)$ or anti-symmetric $(A = -A^T)$?"]},{"cell_type":"code","metadata":{"id":"QrbTBdtoRdTv"},"source":["# Your code here\n","\n","def f(x):\n","  \"\"\"Compute x . A x\"\"\"\n","  pass\n","\n","# Compute the first and second derivatives at a test point x\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"Lg2jplWrRW-n"},"source":["#@title Solution (double-click to show)\n","\n","A = np.array([[1., 0.], [0., 3.]])\n","\n","def f(x):\n","  return jnp.dot(x, jnp.dot(A, x))\n","\n","x = np.array([2., -1.])\n","print(grad(f)(x))\n","print(hessian(f)(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LYZr7ISdgREQ"},"source":["### Exercise: Entropy\n","\n","Compute the Gradient and Hessian of the Shannon entropy:\n","$$S = -\\sum_{i}{x_i \\log x_i}$$\n","\n","Note that for a test point you'll need to have all the elements positive and summing to 1, so a good choice is $$[1 / n, \\ldots, 1 / n]$$\n"]},{"cell_type":"code","metadata":{"id":"TaUdMb8-gPSV"},"source":["## Your code here\n","\n","def entropy(x):\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"VFZVcOW4hBBs"},"source":["#@title Solution (double-click to show)\n","\n","def entropy(x):\n","  return - sum(a * jnp.log(a) for a in x)  \n","\n","x = np.array([1./2, 1./2])\n","print(entropy(x))\n","print(grad(entropy)(x))\n","print(hessian(entropy)(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u4GuEnOW5AMH"},"source":["## Example: Linear Regression with Jax\n","\n","Given some data of the form $(x_i, y_i)$, let's find a best fit line $$ y = m x + b $$ by minimizing the sum of squared errors.\n","\n","$$ S = \\sum_{i}{\\left(y_i - (m x_i + b) \\right)^2}$$\n"]},{"cell_type":"code","metadata":{"id":"Y1LO3kTsU5RZ"},"source":["## Adapted from JAX docs: https://coax.readthedocs.io/en/latest/examples/linear_regression/jax.html\n","\n","# Generate some data using sklearn\n","X, y = make_regression(n_features=1, noise=10)\n","X, X_test, y, y_test = train_test_split(X, y)\n","\n","# Plot the data\n","plt.scatter([x[0] for x in X], y)\n","plt.title(\"Randomly generated dataset\")\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ATPMUaRapgAu"},"source":["Read through the following code, which minimizes the sum of squared errors for a linear model."]},{"cell_type":"code","metadata":{"id":"YMdT41F7UOfK"},"source":["# In JAX, we can specify our parameters as various kinds of Python objects,\n","# including dictionaries.\n","\n","# Initial model parameters\n","params = {\n","    'w': jnp.zeros(X.shape[1:]),\n","    'b': 0.\n","}\n","\n","# The model function itself, a linear function.\n","def forward(params, X):\n","  \"\"\"y = w x + b\"\"\"\n","  return jnp.dot(X, params['w']) + params['b']\n","\n","# The loss function we want to minimize, the sum of squared errors\n","# of the model prediction versus the true values\n","def sse(params, X, y):\n","  \"\"\"Sum of squared errors (mean)\"\"\"\n","  err = forward(params, X) - y\n","  return jnp.mean(jnp.square(err))\n","\n","# Function to update our parameters in each step of gradient descent\n","def update(params, grads, alpha=0.1):\n","    return jax.tree_multimap(lambda p, g: p - alpha * g, params, grads)\n","\n","# We'll define a gradient descent function similarly to as before,\n","# and we'll track the loss function values for plotting\n","# Note also that we compute our gradients on the training data X and y\n","# but our loss function on the test data X_test and y_test\n","def gradient_descent(f, params, X, X_test, y, y_test, alpha=0.1, iterations=30):\n","  \"\"\"\n","  Apply gradient descent to the function f with starting point x_0\n","  and learning rate \\alpha.\n","  \n","  x_{n+1} = x_n - \\alpha d_f(x_n)\n","  \n","  \"\"\"\n","  grad_fn = grad(f)\n","  params_ = []\n","  losses = []\n","\n","  for _ in range(iterations):\n","    grads = grad_fn(params, X, y)\n","    params = update(params, grads, alpha)\n","    params_.append(params)\n","\n","    loss = f(params, X_test, y_test)\n","    losses.append(loss)\n","\n","  return params_, losses\n","\n","# Function to plot our residuals to see how the loss function progresses\n","def plot_residuals(params, model_fn, X, y, color='blue'):\n","  res = y - model_fn(params, X)\n","  plt.hist(res, bins=10, color=color, alpha=0.5)\n","\n","# Find the best fit line\n","fit_params, losses = gradient_descent(sse, params, X, X_test, y, y_test)\n","\n","# Plot the decrease in the loss function over iterations.\n","plt.plot(range(len(losses)), losses)\n","plt.ylabel(\"SSE\")\n","plt.xlabel(\"Iteration\")\n","plt.title(\"Loss evolution\\nMinimizing sum of squared errors\")\n","plt.show()\n","\n","# Compare the errors of our initial guess model with the final model\n","# Note the differences in scales\n","plot_residuals(params, forward, X, y)\n","plt.title(\"Histogram of initial residuals (errors for each point)\")\n","plt.show()\n","\n","plot_residuals(fit_params[-1], forward, X, y, color='green')\n","plt.title(\"Histogram of final residuals (errors for each point)\")\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaV9lNvIW-Rb"},"source":["# Let's plot the best fit line\n","\n","# Plot the data\n","xs = [x[0] for x in X]\n","plt.scatter(xs, y)\n","plt.title(\"Best fit line\")\n","\n","# Plot the best fit line\n","params = fit_params[-1]\n","xs = np.arange(min(xs), max(xs), 0.1)\n","m = float(params['w'])\n","b = float(params['b'])\n","ys = [m * x + b for x in xs] \n","\n","plt.plot(xs, ys, color='black')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J97jl7B8YH3w"},"source":["We can easily use another loss function, like the mean absolute error, where use the absolute value of residuals instead of the square, which reduces the\n","impact of outliers.\n","\n","$$ S = \\sum_{i}{\\left|y_i - (m x_i + b) \\right|}$$\n","\n","This will give us a different best fit line for some data sets."]},{"cell_type":"code","metadata":{"id":"OOGIqdtE6HA_"},"source":["## Minimize MAE instead of SSE\n","\n","# MAE is the p=1 case of this function.\n","def lp_norm(p=2):\n","  def norm(params, X, y):\n","    err = forward(params, X) - y\n","    return jnp.linalg.norm(err, ord=p)\n","  return norm\n","\n","# Generate some noisier data\n","X, y = make_regression(n_features=1, noise=100, bias=5)\n","X, X_test, y, y_test = train_test_split(X, y)\n","\n","fit_params, losses = gradient_descent(lp_norm(p=1.), params, X, X_test, y, y_test)\n","\n","# Plot the data\n","xs = [x[0] for x in X]\n","plt.scatter(xs, y)\n","plt.title(\"Best fit line\")\n","\n","# Plot the best fit line\n","params = fit_params[-1]\n","xs = np.arange(min(xs), max(xs), 0.1)\n","m = float(params['w'])\n","b = float(params['b'])\n","ys = [m*x+b for x in xs] \n","plt.plot(xs, ys, color='black', label=\"MAE\")\n","\n","# Compare to SEE best fit line\n","\n","# Find the best fit line\n","fit_params, losses = gradient_descent(sse, params, X, X_test, y, y_test)\n","\n","# Plot the best fit line\n","params = fit_params[-1]\n","xs = np.arange(min(xs), max(xs), 0.1)\n","m = float(params['w'])\n","b = float(params['b'])\n","ys = [m*x+b for x in xs] \n","plt.plot(xs, ys, color='green', label=\"SSE\")\n","plt.legend()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzybiKWip7zY"},"source":[""],"execution_count":null,"outputs":[]}]}